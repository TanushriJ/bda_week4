---
title: An R Markdown document converted from "quickstart-facial-expression-recognition-bda2021.ipynb"
output: html_document
---

```{r}
## Importing packages

library(tidyverse) # metapackage with lots of helpful functions
library(png)       # package that can be used to read png image files in a simple format
library(parallel)
suppressMessages(require(caret))
suppressMessages(library(doParallel))

KAGGLE_RUN = FALSE
# You can access files the "../input/" directory.
# You can see the files by running  
```

```{r}
# Show the availabe directories
if (KAGGLE_RUN){
  list.files(path = "../input/")
  dirs = dir("../input", pattern="[^g]$", recursive=TRUE, include.dirs = TRUE, full.names = TRUE)
} else {
  dirs = dir(pattern="[^g]$", recursive=TRUE, include.dirs = TRUE, full.names = TRUE)
}
```

```{r}
# Get all image files: file names ending ".png" 
anger   = dir(grep("anger",   dirs, value = TRUE), pattern = "png$", full.names = TRUE)
disgust = dir(grep("disgust", dirs, value = TRUE), pattern = "png$", full.names = TRUE)
happy   = dir(grep("happy",   dirs, value = TRUE), pattern = "png$", full.names = TRUE)
sad     = dir(grep("sad",     dirs, value = TRUE), pattern = "png$", full.names = TRUE)
test_im = dir(grep("test",    dirs, value = TRUE), pattern = "png$", full.names = TRUE)

str(anger)
str(disgust)
str(happy)
str(sad)
str(test_im)
```

The pictures (video stills) show faces that were captured while more or less spontaneously expressing an emotion. Some of the images are repeated, but then shifted, rotated, or both. Not all are easy to classify:

```{r}
if (KAGGLE_RUN){
  ok = file.copy(  happy[60], "happy.png", overwrite = TRUE)
  ok = file.copy(    sad[61],   "sad.png", overwrite = TRUE)
  ok = file.copy(  anger[61], "anger.png", overwrite = TRUE)
  ok = file.copy(disgust[61], "disgust.png", overwrite = TRUE)
  IRdisplay::display_html('<img src="happy.png" width="200" style="float:left" /><img src="sad.png" width="200" style="float:left" /><img src="anger.png" width="200" style="float:left" /><img src="disgust.png" width="200" style="float:left" />')
}
```

Clearly the first is a happy face, but is the second a sad face, an angry face, or both?

# Data considerations

It's not difficult to find out how the data were collected. This is an important question to answer about any data set you use in a machine learning project because it will determine to what new data your model will generalize.

Answer the 3 most important questions for any ML project:

1. Where do the data come from? (To which population will results generalize?)
2. What are candidate machine learning methods? (models? features?)
3. What is the Bayes' error bound? (Any guestimate from scientific literature or web resources?)




<div style=color:darkblue;background-color:#fafaff;min-height:8em; >


<br>
<em>... Double tap to type your team's answers to all three questions here ...</em>
<br>

<!-- Use Markdown or HTML to format your answer -->
    

</div>

# Bayes bound

To have an idea of a lower bound on the Bayes bound (i.e., the minimum accuracy that should be achievable). The best 'machine' we have at hand to recognize emotion from facial expression in the human brain. How often do human judges get it correct? In a study by Mollahosseini et al. (2018) an estimate for human classification inter-rater agreement was obtained for 11 emotions. For the four included in this data set they are:


| disgust  |  anger  |  happy  |  sad  |
|---------:|--------:|--------:|-------|
|  67.6%   | 62.3%   | 79.6%   | 69.7% |


Keep this in mind when evaluating the performance of the classifiers that you'll train.

As always, it's handy to evaluate how the algorithm does on the training set: If the training set is not classified accurately, how can you expect the test set to do any better. This obvious fact is often overlookedâ€”surprisingly.

# Images in R

We'll demonstrate how to process images in R by importing the one of these.

```{r}
img = readPNG(happy[4])
glimpse(img)
```

As you can see the image is stored as a three dimensional array. Why is this? Color images are represented by mixing 3 colors: red, green, and blue. Different intensities of light in these three base colors, mixed together give rise to any conceivable color. Hence, the image are represented in R consists of a 2D matrix of pixel intesities, one for each of these base colors. Let's visualise this

```{r}
# Define a utility function for painting images
paintImage = function(x, colors=1:3, ..., add=FALSE, axes=FALSE) {if(!add) {opar=par(mar=rep(0,4)); image(array(0,dim(x)[1:2]), ..., col="transparent", axes=axes); par(opar)}; x[,,-colors]=0; usr=par('usr'); rasterImage(x, usr[1], usr[3], usr[2], usr[4], ...)}


options(repr.plot.width = 5, repr.plot.height = 5)

# Visualize the structure of color images
layout(matrix(1:4,2))
paintImage(img)
paintImage(img, colors=1)
paintImage(img, colors=2)
paintImage(img, colors=3)
```

The different color dimensions are often called *color channels*. Now the pictures in the data set for this competition are gray scale (but nevertheless stored as color images), and so this seems a little superfluous. Let's look at another picture. At the same time we will look at another color channel that the PNG format also supports: the alpha channel, which defines the transparency of a pixel. 

```{r}
# Download a semi transparent picture of an elephant
download.file("https://i.imgur.com/OKAeTdP.png", destfile = "semitransp.png")
elephant = readPNG("semitransp.png")

str(elephant)
```

Note that the third dimenions of the array has now 4 entries. The last entry is the alpha channel. For each pixel in the picture `elephant[,, 4]` specifies the amount of transparency (0=fully transparent, 1=fully opaque).

```{r}

# Draw the elephant itself, and as an overlay over a scatterplot
options(repr.plot.width = 3*5, repr.plot.height = 3)
layout(t(1:5))

# Elephant itself
paintImage(elephant, colors=1:4) 
paintImage(elephant, colors=c(1,4)) 
paintImage(elephant, colors=c(2,4)) 
paintImage(elephant, colors=c(3,4)) 

# Elephant as an overlay
plot(runif(100), runif(100))
rasterImage(elephant, 0, 0, 1, 1)
```

Now that you have an idea of how images are represented in terms of pixel intensities in three different color channels, you can start thinking about what kind of features you might construct from these pixel values that allow you to classify this images into their respective class.


<img src="https://i.imgur.com/scE5XWh.png" width=300 style="float:right" />

# Feature extraction for images



So let's look at feature extraction. We discuss three approaches to feature extraction from images: 


1. Direct pixel analysis
    - Use pixel intensities are features
        - possibly resize images to reduce number of potential features (i.e., account for pixel correlations)
        - make sure humans can still perform the task with chosen size
    - Akin [MNIST and fashion-MNIST practical](https://www.kaggle.com/datasniffer/example-image-data-mnist)

<br />


2. Similar approach as phone sensor signals:

    <img src="https://i.imgur.com/qc6JIST.png" width=100 style="float:right" />

    - Compute descriptive statistics of the histograms:
        - raw bin counts
        - mean (per color channel)
        - standard deviation
        - shifted correlations (or inner product)
        - "gradients" (so called HoG features: Histograms of Gradients, popular for detecting humans)
        - SIFT features (too complicated to discuss here)
        - spectral measures
    - **Key issues**: scale, shift and *rotation* invariance, and intensity invariance
    - Especially useful for edge coordinates
    - Break up picture into patches (analogeous sub-epochs of phone sensor signals)
    - Akin the Frey-Slate features in the [letter recognition practical](https://www.kaggle.com/datasniffer/k-nn-and-similarity-in-letter-recognition)

<img src="https://image.slidesharecdn.com/98f7f5b1-28c5-4385-a77b-368f27f95cd8-150419012246-conversion-gate01/95/lecture-21-image-categorization-computer-vision-spring2015-43-638.jpg?cb=1429406714" width=400 style="float:right;margin-left:50px" />

3. Bag-of-features models:

    - Use a "dictionary" of pattern patches and count how often each pattern patch 'occurs' 
        - has a match to a high degree in the image, thresholding
        - use the counts (i.e., histograms) of these dictionary patches as features
    - Where to find such a dictionary? 
        - Extract from images themselves (just like in text processing: tokens are obtained from the available texts) 
            - Use clustering methods
        - Use pretrained "convolutional neural nets" 
            - trained for specific recognition tasks
            - trained for image generation
            
The 3rd method is tricky and computationally heavy, because the entire image has to be searched for a match of the "dictionary" of patterns. Although the Fast Fourier Transform (FFT; in two dimensions) offers a fast way to do this, it requires careful programming to implement this effectively. Convolutional neural network libraries indeed use FFT to efficiently implement this in specialized software. We've encountered FFT when we discussed the spectrum of signals.

# Import data

When working with image data, you often have many more Gigabytes of raw data than you have RAM memory available. Therefore, it is often not possible to work with all data "in memory". Resizing images often helps, but may cause loss of information.

The images for this competition are

- gray scale, so we need only one *color channel* 
- are only 48 by 48 pixels

Furthermore there are only 2538 pictures in the training set. Therefore, we are lucky enough to be able to retain all images in RAM, and don't have to do "special stuff" to handle reading in image files while fitting a model.

Reading in images pixelwise is easiest: We simply store each image as a long vector of pixel intensities, row by row. Also we will need a vector that contains the emotion label for each of the images.

```{r}
# Combine all filenames into a single vector
train_image_files = c(anger, happy, sad, disgust)

# Read in the images as pixel values (discarding color channels)
X = sapply(train_image_files, function(nm) c(readPNG(nm)[,,1])) %>% t() 
y = c(rep("anger", length(anger)), rep("happy", length(happy)), rep("sad", length(sad)), rep("disgust", length(disgust)))

X_test = sapply(test_im, function(nm) c(readPNG(nm)[,,1])) %>% t() 


# Change row and column names of X to something more managable
rownames(X)      = gsub(".+train/", "", rownames(X))
rownames(X_test) = gsub(".+test/",  "", rownames(X_test))

colnames(X) = colnames(X_test) = paste("p",1:ncol(X), sep="")

# Check result (are X, X_test, and y what we expect)
X[1:6,20:23] %>% print
table(y)
                
X_test[1:6,20:23] %>% print
```

```{r}
# Visualization utility function
as_image = function(x, nr=sqrt(length(x))) {opar=par(mar=rep(0,4)); on.exit(par(opar)); image(t(matrix(x,nr))[,nr:1], col = gray(0:255/255),axes=F)}


options(repr.plot.width=4, repr.plot.height=4)
as_image(X[13,])
as_image(X_test[13,])
```

# Histogram features from edges

Histogram features work best on edges. How can you detect edges? To answer that question we have to consider what an edge is: An edge is a rapid change in pixel intensities, so if we compute the difference between two consecutive pixels, and check if it is larger than a certain threshold, we can find the pixels that are at the edge of an abrupt intensity change. In principle this can be done in any possible direction (north, south, west, east, north-west, south-west, etc.), but it turns out that it is enough to do it in only two directions: north and west (or south and east for that matter). These give horizontal and vertical edges respectively. By computing differences in both north and west direction consecutively we filter out pixels that are part of a diagonal edge. 

```{r}
options(repr.plot.width=4*4, repr.plot.height=4)

# Compute edges by differencing neighboring pixels
im = matrix(X[756,],48)
h_edge = im[-1,] - im[-48,] # horizontal
v_edge = im[,-1] - im[,-48] # vertical
d_edge = h_edge[,-1] - h_edge[,-48] # diagonal

# Specify a threshold (hand tuned here on visual result)
threshold = .0625 

layout(t(1:4))
as_image(im)
as_image(h_edge < threshold,   47); mtext("horizontal edge pixels")
as_image(v_edge < threshold,   48); mtext("vertical edge pixels")
as_image(d_edge < threshold/2, 47); mtext("diagonal edge pixels")
#as_image((h_edge[,-1] < 0.1) & (v_edge[-1,] < 0.1), 47); mtext("edge pixels")
```

You can use the detected edge pixels to compute Frey and Slate type features: Histogram descriptives of the x and y locations of the 'on' pixels in the edge pixel maps.

```{r}
# Load FreySlateFeatures function 
source("https://bit.ly/32um24j")

FreySlateFeatures(h_edge < threshold)
```

Note that Frey & Slate features were designed for distinguishing capital characters; not for distinguishing emotions in pictures of human faces. If you want to go this handicraft way of feature extraction you may want to consider chosing different histogram descriptors.

You will notice that not all features computed this way have non-zero variance, and it is generally dificult to predict which features are highly correlated or multi-collinear.

# Fit a model

To figure out which model provides the best trade off between bias and variance, between accuracy and flexibility, one strategy is to fit both a flexible and a more rigid model and determine from CV error which direction on the flexiblity axis we should go to avoid overtraining.

We'll consider classification trees and random forests here. Random forests are probably the least susceptible to overtraining and is considered one of the best "off the shelf" machine learning algorithms in the sense that they require little expertise in application, and easily perform well without tuning. (This is not to say that tuning should not be considered!) It's your uncle Bob's prefered algorithm so to say.


## Classification tree

As an example, here we fit a classification tree, using the pixel based approach.

```{r}
(detectCores() - 1) %>% makeCluster() %>% registerDoParallel()


## Fit a CART using 5-fold cross-validation to tune the complexity parameter
set.seed(2020) # for repeatability (generally don't do this!)
trCntrl = trainControl('cv', 5, allowParallel = TRUE)
tt <- Sys.time()
fittree = train(x=X, y=y, method='rpart', trControl = trCntrl, tuneGrid = data.frame(cp=.02))
fittree
#plot(fittree)
(dur <- Sys.time() - tt)
```

```{r}
## Graphical visualization of the decision tree
options(repr.plot.width=14, repr.plot.height=8)
plot(fittree$final, compress=TRUE, uniform=TRUE, margin=0.05, branch=.75); 
text(fittree$final, cex=0.8, all=TRUE, use.n=TRUE)

## Textual visualization of the decision tree
fittree$finalModel
```

The cross validated accuracy estimate is around 55%.

```{r}
## Check performance on training set
predtree = predict(fittree, X, type='raw') 
confusionMatrix(predtree, factor(y))
```

# Formatting your submission file

To format your submission file, you can use the following code:

```{r}
## Make predictions
predtree = predict(fittree, X_test, type='raw')

## Write to file
tibble(file = rownames(X_test), category = predtree) %>% 
    write_csv(path = "submission.csv")

## Check result
cat(readLines("submission.csv",n=20), sep="\n")
```

```{r}

```

